**Алерты в Grafana** были настроены через **Prometheus Alertmanager**.

**Что мониторили?**
	1.     **Загрузка CPU и RAM** – если процессор загружен **>80%** или памяти осталось **<500MB**, отправляется предупреждение.
	2.     **Количество запросов (RPS)** – если RPS резко падает, значит, есть проблема (например, **меньше 50% от среднего значения за час**).
	3.     **Время ответа API (latency)** – если **p95 > 500ms**, алерт.
	4.     **Ошибки (5xx)** – если **ошибок больше 2% от общего числа запросов**, значит, что-то пошло не так.
	5.     **Очереди Kafka** – если сообщений в очереди **>10K**, значит, консьюмеры не успевают обрабатывать.
	6.     **База данных** – если время выполнения SQL-запросов **>200ms**, возможно, нужен индекс или есть блокировки.

**Какие пороги устанавливали?**
	·         **Warning (предупреждение)** – если нагрузка приближается к критическим значениям.
	·         **Critical (критический алерт)** – если система уже на грани.

**Как приходили алерты?**
	1.     **Slack** – в специальный канал с разработчиками и DevOps.
	2.     **Email** – для критических проблем.
	3.     **PagerDuty** – если инцидент срочный, отправляется SMS или звонок ответственному инженеру.

Настройки регулярно пересматривались, чтобы не было ложных срабатываний.